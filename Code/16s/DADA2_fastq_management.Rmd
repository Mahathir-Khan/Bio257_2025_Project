---
title: "DADA2_fastq_management"
author: "Aman"
date: "2025-11-04"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(dada2)
```

```{r}
#using path to locate and extract forward and reverse files
path <- "/gpfs/fs2/scratch/bio257_2025/Users/group4_ADxTummy/16s_data/fastq.reads"

fnFs <- sort(list.files(path, pattern="_1.fastq.gz", full.names = TRUE))

fnRs <- sort(list.files(path, pattern = "_2.fastq.gz",full.names = TRUE))

sample.names <- sapply(strsplit(basename(fnFs),"_"), '[', 1)
```

```{r}
# checking read quality
plotQualityProfile(fnFs[1:2])

plotQualityProfile(fnRs[1:2])
```

```{r}
# placing filtered files in subdirectory
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))

filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))

names(filtFs) <- sample.names
names(filtRs) <- sample.names

out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs,truncLen=c(240,160),
                     maxN=0, maxEE=c(4,2), truncQ = 2,
                     rm.phix = TRUE, compress = TRUE, multithread = TRUE)

head(out)
```

```{r}
# Creating error model and learn error rates
# will be utilized will be used to account for error in sampling algorithm
errF <- learnErrors(filtFs, multithread = TRUE)

errR <- learnErrors(filtRs, multithread = TRUE)
```

```{r}
# applying core sample inference algorithm (from dada) to filtered and tripmmed 
dadaFs <- dada(filtFs, err=errF, multithread = TRUE)

dadaRs <- dada(filtRs, err=errR, multithread = TRUE)

dadaFs[[1]]
```

```{r}
# aligning and merging all forward and reverse reads
# provides full denoised sequences
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose = TRUE)

# just inspecting first sample merge
head(mergers[[1]])
```

```{r}
# Constructing asv table
seqtab <- makeSequenceTable(mergers)
dim(seqtab)

# reviewing distribution of sequence lengths
table(nchar(getSequences(seqtab)))
```

```{r}
# remove chimeras by combing left and right segments of parents and comaring across reads
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread = TRUE,
                                    verbose = TRUE)
dim(seqtab.nochim)

sum(seqtab.nochim)/sum(seqtab)
```

```{r}
# tracking reads across pipline just to check how things changed
# as we created asv
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN),
               sapply(mergers, getN), rowSums(seqtab.nochim))

colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR",
                     "merged", "nonchim")

rownames(track) <- sample.names
head(track)
```

```{r}
# reformatting table to dataframe, renaming rows to match out analysis
# pipeline formatting, and exporting file for analysis
asv_table <- as.data.frame(seqtab.nochim)

asv_table <- cbind(SampleID = rownames(asv_table), asv_table)

write.table(asv_table, "asv_table.tsv", sep="\t", quote = FALSE, row.names = FALSE)
```


